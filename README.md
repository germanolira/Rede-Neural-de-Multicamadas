# Rede Neural de Multicamadas ü§ñüëæ

[![Build Status](https://travis-ci.com/germanolira/Rede-Neural-de-Multicamadas.svg?branch=master)](https://travis-ci.com/germanolira/Rede-Neural-de-Multicamadas)

 Uma melhora na rede neural simples, adicionando mais camadas e dificultando o problema

Ha mais ou menos um ano atr√°s fiz um fork de uma rede neural simples e melhorei o c√≥digo, atualizei para o Python 3, traduzi e postei, acabei aprendendo como criar uma rede neural simples para resolver um problema de prever n√∫meros em uma tabela, agora com um problema mais dif√≠cil na qual aquela rede neural n√£o conseguiria resolver.

# O problema

![Image description](https://miro.medium.com/max/682/1*WlCd6kgxqb2yQpcutLhYwQ.png)

O truque √© perceber que a terceira coluna √© irrelevante, mas as duas primeiras colunas exibem o comportamento de uma porta XOR . Se a primeira coluna ou a segunda coluna for 1, a sa√≠da ser√° 1. No entanto, se as duas colunas forem 0 ou as duas colunas forem 1, a sa√≠da ser√° 0.
Portanto, a resposta correta √© 0.
No entanto, isso seria demais para o nosso √∫nico neur√¥nio suportar. Isso √© considerado um "padr√£o n√£o linear", porque n√£o h√° rela√ß√£o direta direta entre as entradas e a sa√≠da.
Em vez disso, devemos criar uma camada oculta adicional, composta por quatro neur√¥nios (Camada 1). Essa camada permite que a rede neural pense em combina√ß√µes de entradas.

![Image description](https://miro.medium.com/max/423/1*Qt5lealRQ29-R8rcTPDtoA.png)

Voc√™ pode ver no diagrama que a sa√≠da da camada 1 √© alimentada na camada 2. Agora √© poss√≠vel para a rede neural descobrir correla√ß√µes entre a sa√≠da da camada 1 e a sa√≠da no conjunto de treinamento. Conforme a rede neural aprende, amplificar√° essas correla√ß√µes ajustando os pesos nas duas camadas.
De fato, o reconhecimento de imagem √© muito semelhante. N√£o h√° rela√ß√£o direta entre pixels e ma√ß√£s. Mas h√° uma rela√ß√£o direta entre combina√ß√µes de pixels e ma√ß√£s.

![Image description](https://miro.medium.com/max/160/1*YqjgIOW86wioEhmZeWbcqw.jpeg)

O processo de adicionar mais camadas a uma rede neural, para que ela possa pensar em combina√ß√µes, √© chamado de "aprendizado profundo".Ok, estamos prontos para o c√≥digo Python? Primeiro vou dar o c√≥digo e depois vou explicar mais.

# Aqui estaria o c√≥digo, voc√™ pode olhar dentro do arquivo "aprendizado_profundo.py"

O que √© diferente desta vez √© que existem v√°rias camadas. Quando a rede neural calcula o erro na camada 2, ela propaga o erro para tr√°s na camada 1, ajustando os pesos √† medida que avan√ßa. Isso √© chamado de "propaga√ß√£o traseira".

![Image description](https://miro.medium.com/max/840/1*-1trgA6DUEaafJZv3k0mGw.jpeg)

# Depend√™ncias

- pip install numpy
- python3 main.py

Cr√©ditos: Milo Spencer-Harper

@miloharper :+1: Thanks, Mr. Spencer, awesome project! :shipit:

# Alguns Links

- https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a
- https://github.com/miloharper/multi-layer-neural-network

> Eu acabei n√£o fazendo um fork por um simples motivo: acabei criando um reposit√≥rio e esqueci de fazer um fork
